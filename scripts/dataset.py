# -*- coding: utf-8 -*-
"""shark-initialseti.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oUGEC-RPu9DxnDLidVM2su46DVpjjnpV

# Installs and Imports
"""
print("Starting!")
# install dependencies:
# (use +cu100 because colab is on CUDA 10.0)
# !pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html 
# !pip install cython pyyaml==5.1
# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
# import torch, torchvision
import torch
# torch.__version__
# !gcc --version
# opencv is pre-installed on colab

# install detectron2:
# !git clone https://github.com/facebookresearch/detectron2 detectron2_repo
# !pip install -e detectron2_repo

# You may need to restart your runtime prior to this, to let your installation take effect
# Some basic setup:
# Setup detectron2 logger
# import detectron2

# import some common libraries
import numpy as np
import cv2
# import random
# import math

import os
import json
from detectron2.structures import BoxMode

# import some common detectron2 utilities
# from detectron2 import model_zoo
# from detectron2.engine import DefaultPredictor
# from detectron2.config import get_cfg
# from detectron2.utils.visualizer import Visualizer
# from detectron2.data import MetadataCatalog
import argparse

print("Done importing!")

parser = argparse.ArgumentParser(
    description="Create the CSV",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument(
  "-d",
  "--dataset",
  default="s",
  type=str,
  help="The dataset being used."
)

dataset_used = ""
if(parser.parse_args().dataset == "s"):
  dataset_used = "small"
  print("Dataset being used is the small dataset")
elif(parser.parse_args().dataset == "l"):
  dataset_used = "large"
  print("Dataset being used is the large dataset")
else:
  raise ValueError("Dataset arg provided \""+parser.parse_args().dataset+"\" is invalid")


"""# Directories"""

trainDirectory      = ""
valDirectory        = ""
imageDirectory      = ""
sourceJsonDirectory = ""
baseDirectory       = ""
baseOutputDirectory = ""

if(dataset_used == "small"):
  trainDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/train/"
  valDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/val/"
  imageDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/images/"
  sourceJsonDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/data.json"
  baseDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/"
  baseOutputDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/outputs/small/"
if(dataset_used == "large"):
  trainDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/train/"
  valDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/val/"
  imageDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/images/"
  sourceJsonDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/data.json"
  baseDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/"
  baseOutputDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/outputs/large/"

"""# Dataset

Construct Dictionary of SharkIDs to Classes
"""

# os.mkdir("hi")
# os.getcwd("hi")
# print("made it")

def createSharkClassID(filename):
  json_file = filename

  SharkClassDictionary = {}

  # Open the file
  with open(json_file) as f:
    img_annotations = json.load(f)

  classID = 0
  for i, values in enumerate(img_annotations):
    # Get the sharkID out
    sharkID   = values["id"]

    # If the current sharkID has not been assigned a classID
    if(sharkID not in SharkClassDictionary):
      # Add the sharkID to the dictionary, giving it a classID
      SharkClassDictionary[sharkID] = classID
      # Increment the classID for the next sharkID
      classID = classID + 1

  return SharkClassDictionary

# Generate dictionary
# SharkClassDictionary = createSharkClassID("data/train/data.json")
SharkClassDictionary = createSharkClassID(sourceJsonDirectory)
# print(SharkClassDictionary)
torch.save(SharkClassDictionary,baseDirectory+"SharkClassDictionary.pt")

"""Create Class List"""

ClassList = [""] * len(SharkClassDictionary)
for sharkID in list(SharkClassDictionary):
  ClassList[SharkClassDictionary[sharkID]] = sharkID

# print(ClassList)
torch.save(ClassList,baseDirectory+"ClassList.pt")

"""Registering the shark data with detectron2"""

# Construct the shark dictionaries
# This function will return the items in our dataset
def constructSharkDicts(dataDirectory):
  print("Constructing shark dicts")

  # Find the json file in our file structure
  json_file = os.path.join(dataDirectory, "data.json")

  # Open the file, referring to it as f
  with open(json_file) as f:
    # Get the image annotations out using json to read the file containin the JSON object
    img_annotations = json.load(f)

  # List of dictionaries
  dataset_dicts = []
  
  # Loop over the image annotation values, using idx as the indexer and v to be the value
  for i, values in enumerate(img_annotations):
    # Print every 100 iteration for time tracking
    if(dataset_used == "small"): 
      if(i % 100 == 0): print(i)
    if(dataset_used == "large"): 
      if(i % 1000 == 0): print(i)

    # if(i == 5000): break

    # Create empty record
    record = {}

    # Extract information from json file
    imageID   = values["imId"]
    filename = imageDirectory + values["imId"] +".jpg"
    sharkID   = values["id"]
    # sharkSide = values["side"]

    # No bounding box given: use triangle
    if(dataset_used == "small"):
      keypoints = values["keypoints"]
      tip_yx = keypoints["tip_yx"]
      leading_yx = keypoints["leading_yx"]
      trailing_yx = keypoints["trailing_yx"]
    
      # Create segmentation polygon
      # poly = [tip_yx[1],tip_yx[0], leading_yx[1],leading_yx[0], trailing_yx[1],trailing_yx[0]]

      # Get the bounding box dimensions
      xs = [tip_yx[1], leading_yx[1], trailing_yx[1]]
      ys = [tip_yx[0], leading_yx[0], trailing_yx[0]]
      xmax = (max(xs) + 5)
      ymax = (max(ys) + 5)
      xmin = (min(xs) - 5)
      ymin = (min(ys) - 5)

    if(dataset_used == "large"):
      ymin, xmin, ymax, xmax = values["box_ymin_xmin_ymax_xmax"]

    # Check image exists (might not because of how I'm handling dataset)
    if(not os.path.isfile(filename)): continue

    # Get image height and width
    imHeight, imWidth = cv2.imread(filename).shape[:2]

    # Construct annotation object (bounding box)
    objs = []
    
    #Get ClassID from SharkID
    classID = SharkClassDictionary[sharkID]

    # Construct an annotation object (only one in this case)
    obj = \
    {
        "bbox": [xmin,ymin,xmax,ymax],
        "bbox_mode": BoxMode.XYXY_ABS,
        # "sementation": [poly],
        "category_id": classID, #this needs to be an INTEGER ID for the shark's category, so we have created a mapping from sharkIDs to classIDs
        "iscrowd": 0
    }
    objs.append(obj)

    # Populate record
    record["file_name"] = filename
    record["image_id"]  = imageID
    record["height"]    = imHeight
    record["width"]     = imWidth
    record["annotations"] = objs

    # Append this record to the list of dictionaries
    dataset_dicts.append(record)
    
    # Only do it for 3 images (time saving)
    # print()
    # if i == 2:
      # break

  # Return the list of dictionaries
  return dataset_dicts

tmpDict = constructSharkDicts(trainDirectory)#"/content/drive/My Drive/sharkdata/initial/images/train/")
# torch.save(tmpDict,"/content/drive/My Drive/sharkdata/initial/images/sharkTrainDicts.pt")
torch.save(tmpDict,trainDirectory+"/sharkTrainDicts.pt")

# tmpDict = constructSharkDicts("/content/drive/My Drive/sharkdata/initial/images/val/")
tmpDict = constructSharkDicts(valDirectory)
# torch.save(tmpDict,"/content/drive/My Drive/sharkdata/initial/images/sharkValDicts.pt")
torch.save(tmpDict,valDirectory+"/sharkValDicts.pt")
