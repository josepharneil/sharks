{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sharktraintest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7IEYPte6jom",
        "colab_type": "text"
      },
      "source": [
        "# Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8I78pC26m9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install dependencies:\n",
        "# (use +cu100 because colab is on CUDA 10.0)\n",
        "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "!pip install cython pyyaml==5.1\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "import torch, torchvision\n",
        "torch.__version__\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-oRr5jE6n4U",
        "colab_type": "code",
        "outputId": "789c45cb-b0c7-4a19-ecc6-2732cf25674d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "source": [
        "# install detectron2:\n",
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'detectron2_repo' already exists and is not an empty directory.\n",
            "Obtaining file:///content/detectron2_repo\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.1.0)\n",
            "Requirement already satisfied: Pillow==6.2.2 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (6.2.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.1.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.8.6)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (4.42.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.15.0)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.1.dev200114)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2==0.1) (1.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2==0.1) (5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2==0.1) (1.17.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (42.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (3.1.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2==0.1) (0.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from fvcore->detectron2==0.1) (1.5.2)\n",
            "Installing collected packages: detectron2\n",
            "  Found existing installation: detectron2 0.1\n",
            "    Can't uninstall 'detectron2'. No files were found to uninstall.\n",
            "  Running setup.py develop for detectron2\n",
            "Successfully installed detectron2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhtGJJHa6rbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You may need to restart your runtime prior to this, to let your installation take effect\n",
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUTyxH7g8gak",
        "colab_type": "code",
        "outputId": "d1da5dcb-cb81-494d-f35d-6b4083888da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAxfgpfWzvvS",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pBMURbpBDyT",
        "colab_type": "text"
      },
      "source": [
        "Construct Dictionary of SharkIDs to Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYTYtienwvB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createSharkClassID(filename):\n",
        "  json_file = filename\n",
        "\n",
        "  SharkClassDictionary = {}\n",
        "\n",
        "  # Open the file\n",
        "  with open(json_file) as f:\n",
        "    img_annotations = json.load(f)\n",
        "\n",
        "  classID = 0\n",
        "  for i, values in enumerate(img_annotations):\n",
        "    # Get the sharkID out\n",
        "    sharkID   = values[\"id\"]\n",
        "\n",
        "    # If the current sharkID has not been assigned a classID\n",
        "    if(sharkID not in SharkClassDictionary):\n",
        "      # Add the sharkID to the dictionary, giving it a classID\n",
        "      SharkClassDictionary[sharkID] = classID\n",
        "      # Increment the classID for the next sharkID\n",
        "      classID = classID + 1\n",
        "\n",
        "  return SharkClassDictionary\n",
        "\n",
        "# Generate dictionary\n",
        "# SharkClassDictionary = createSharkClassID(\"data/train/data.json\")\n",
        "SharkClassDictionary = createSharkClassID(\"/content/drive/My Drive/sharkdata/all_data/train/data.json\")\n",
        "# SharkClassDictionary = createSharkClassID(\"/content/drive/My Drive/sharkdata/train/data.json\")\n",
        "# print(SharkClassDictionary)\n",
        "# print(list(SharkClassDictionary.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL6FNLYl1iBF",
        "colab_type": "text"
      },
      "source": [
        "Create Class List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgykSfsY1g6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ClassList = [\"\"] * len(SharkClassDictionary)\n",
        "for sharkID in list(SharkClassDictionary):\n",
        "  ClassList[SharkClassDictionary[sharkID]] = sharkID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw6rSXCe0QjK",
        "colab_type": "text"
      },
      "source": [
        "Registering the shark data with detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fj907NEiBhs",
        "colab_type": "code",
        "outputId": "c9a1b3dd-9aff-41c7-cb1b-29a7be5ddda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "# Construct the shark dictionaries\n",
        "# This function will return the items in our dataset\n",
        "def constructSharkDicts(dataDirectory):\n",
        "  print(\"Constructing shark dicts\")\n",
        "\n",
        "  # Find the json file in our file structure\n",
        "  json_file = os.path.join(dataDirectory, \"data.json\")\n",
        "\n",
        "  # Open the file, referring to it as f\n",
        "  with open(json_file) as f:\n",
        "    # Get the image annotations out using json to read the file containin the JSON object\n",
        "    img_annotations = json.load(f)\n",
        "\n",
        "  # List of dictionaries\n",
        "  dataset_dicts = []\n",
        "  \n",
        "  # Loop over the image annotation values, using idx as the indexer and v to be the value\n",
        "  for i, values in enumerate(img_annotations):\n",
        "    # Print every 100 iteration for time tracking\n",
        "    if(i % 1000 == 0): print(i)\n",
        "\n",
        "    # if(i == 5000): break\n",
        "\n",
        "    # Create empty record\n",
        "    record = {}\n",
        "\n",
        "    # Extract information from json file\n",
        "    imageID   = values[\"imId\"]\n",
        "    filename  = dataDirectory + \"/\" + values[\"imId\"] +\".jpg\"\n",
        "    sharkID   = values[\"id\"]\n",
        "    sharkSide = values[\"side\"]\n",
        "    ymin, xmin, ymax, xmax = values[\"box_ymin_xmin_ymax_xmax\"]\n",
        "\n",
        "    # Check image exists (might not because of how I'm handling dataset)\n",
        "    if(not os.path.isfile(filename)): continue\n",
        "\n",
        "    # Get image height and width\n",
        "    imHeight, imWidth = cv2.imread(filename).shape[:2]\n",
        "\n",
        "    # Construct annotation object (bounding box)\n",
        "    objs = []\n",
        "    \n",
        "    #Get ClassID from SharkID\n",
        "    classID = SharkClassDictionary[sharkID]\n",
        "\n",
        "    # Construct an annotation object (only one in this case)\n",
        "    obj = \\\n",
        "    {\n",
        "        \"bbox\": [xmin,ymin,xmax,ymax],\n",
        "        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "        \"category_id\": classID, #this needs to be an INTEGER ID for the shark's category, so we have created a mapping from sharkIDs to classIDs\n",
        "        \"iscrowd\": 0\n",
        "    }\n",
        "    objs.append(obj)\n",
        "\n",
        "    # Populate record\n",
        "    record[\"file_name\"] = filename\n",
        "    record[\"image_id\"]  = imageID\n",
        "    record[\"height\"]    = imHeight\n",
        "    record[\"width\"]     = imWidth\n",
        "    record[\"annotations\"] = objs\n",
        "\n",
        "    # Append this record to the list of dictionaries\n",
        "    dataset_dicts.append(record)\n",
        "    \n",
        "    # Only do it for 3 images (time saving)\n",
        "    # print()\n",
        "    # if i == 2:\n",
        "      # break\n",
        "\n",
        "  # Return the list of dictionaries\n",
        "  return dataset_dicts\n",
        "\n",
        "tmpDict = constructSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/train/\")\n",
        "torch.save(tmpDict,\"/content/drive/My Drive/sharkdata/all_data/train/sharkTrainDicts.pt\")\n",
        "\n",
        "tmpDict = constructSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/val/\")\n",
        "torch.save(tmpDict,\"/content/drive/My Drive/sharkdata/all_data/val/sharkValDicts.pt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constructing shark dicts\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a3e69174eed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset_dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtmpDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructSharkDicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/sharkdata/all_data/train/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/drive/My Drive/sharkdata/all_data/train/sharkTrainDicts.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a3e69174eed8>\u001b[0m in \u001b[0;36mconstructSharkDicts\u001b[0;34m(dataDirectory)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Get image height and width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mimHeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimWidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Construct annotation object (bounding box)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8hzOjTmzxdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # This function will return the items in our dataset\n",
        "# def getSharkDicts(dataDirectory):\n",
        "#   dataset_dicts = torch.load(dataDirectory)\n",
        "#   return dataset_dicts\n",
        "\n",
        "\n",
        "def getSharkTrainDicts():\n",
        "  return torch.load(\"/content/drive/My Drive/sharkdata/all_data/train/sharkTrainDicts.pt\")\n",
        "\n",
        "def getSharkValDicts():\n",
        "  return torch.load(\"/content/drive/My Drive/sharkdata/all_data/val/sharkValDicts.pt\")\n",
        "\n",
        "\n",
        "\n",
        "  # print(\"Getting shark dicts\")\n",
        "\n",
        "  # # Find the json file in our file structure\n",
        "  # json_file = os.path.join(dataDirectory, \"data.json\")\n",
        "\n",
        "  # # Open the file, referring to it as f\n",
        "  # with open(json_file) as f:\n",
        "  #   # Get the image annotations out using json to read the file containin the JSON object\n",
        "  #   img_annotations = json.load(f)\n",
        "\n",
        "  # # List of dictionaries\n",
        "  # dataset_dicts = []\n",
        "  \n",
        "  # # Loop over the image annotation values, using idx as the indexer and v to be the value\n",
        "  # for i, values in enumerate(img_annotations):\n",
        "  #   # Print every 100 iteration for time tracking\n",
        "  #   if(i % 1000 == 0): print(i)\n",
        "\n",
        "  #   # if(i == 5000): break\n",
        "\n",
        "  #   # Create empty record\n",
        "  #   record = {}\n",
        "\n",
        "  #   # Extract information from json file\n",
        "  #   imageID   = values[\"imId\"]\n",
        "  #   filename  = dataDirectory + \"/\" + values[\"imId\"] +\".jpg\"\n",
        "  #   sharkID   = values[\"id\"]\n",
        "  #   sharkSide = values[\"side\"]\n",
        "  #   ymin, xmin, ymax, xmax = values[\"box_ymin_xmin_ymax_xmax\"]\n",
        "\n",
        "  #   # Check image exists (might not because of how I'm handling dataset)\n",
        "  #   if(not os.path.isfile(filename)): continue\n",
        "\n",
        "  #   # Get image height and width\n",
        "  #   imHeight, imWidth = cv2.imread(filename).shape[:2]\n",
        "\n",
        "  #   # Construct annotation object (bounding box)\n",
        "  #   objs = []\n",
        "    \n",
        "  #   #Get ClassID from SharkID\n",
        "  #   classID = SharkClassDictionary[sharkID]\n",
        "\n",
        "  #   # Construct an annotation object (only one in this case)\n",
        "  #   obj = \\\n",
        "  #   {\n",
        "  #       \"bbox\": [xmin,ymin,xmax,ymax],\n",
        "  #       \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "  #       \"category_id\": classID, #this needs to be an INTEGER ID for the shark's category, so we have created a mapping from sharkIDs to classIDs\n",
        "  #       \"iscrowd\": 0\n",
        "  #   }\n",
        "  #   objs.append(obj)\n",
        "\n",
        "  #   # Populate record\n",
        "  #   record[\"file_name\"] = filename\n",
        "  #   record[\"image_id\"]  = imageID\n",
        "  #   record[\"height\"]    = imHeight\n",
        "  #   record[\"width\"]     = imWidth\n",
        "  #   record[\"annotations\"] = objs\n",
        "\n",
        "  #   # Append this record to the list of dictionaries\n",
        "  #   dataset_dicts.append(record)\n",
        "    \n",
        "  #   # Only do it for 3 images (time saving)\n",
        "  #   # print()\n",
        "  #   # if i == 2:\n",
        "  #     # break\n",
        "\n",
        "  # # Return the list of dictionaries\n",
        "  # return dataset_dicts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-q9-Mxzj3VU",
        "colab_type": "code",
        "outputId": "82b73326-007a-4f08-e6e9-143ddd99cf88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "# Enter into dataset and metadata catalogues\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "# for d in [\"train\", \"val\"]:\n",
        "#   # Register shark_dictionary\n",
        "#   # DatasetCatalog.register(\"shark_\" + d, lambda d=d: getSharkDicts(\"data/\" + d))\n",
        "#   # DatasetCatalog.register(\"shark_\" + d, lambda d=d: getSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/\" + d))\n",
        "#   DatasetCatalog.register(\"shark_\" + d, lambda d=d: getSharkDicts(\"/dicts/\" + d))\n",
        "#   # DatasetCatalog.register(\"shark_\" + d, lambda d=d: getSharkDicts(\"/content/drive/My Drive/sharkdata/\" + d))\n",
        "#   # MetadataCatalog.get(\"shark_\" + d).set(thing_classes=list(SharkClassDictionary.keys()))\n",
        "#   MetadataCatalog.get(\"shark_\" + d).set(thing_classes=ClassList)\n",
        "\n",
        "# DatasetCatalog.register(\"shark_train\", getSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/train\"))\n",
        "DatasetCatalog.register(\"shark_train\", getSharkTrainDicts())\n",
        "MetadataCatalog.get(\"shark_train\").set(thing_classes=ClassList)\n",
        "# DatasetCatalog.register(\"shark_val\", getSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/val\"))\n",
        "DatasetCatalog.register(\"shark_val\", getSharkValDicts())\n",
        "MetadataCatalog.get(\"shark_val\").set(thing_classes=ClassList)\n",
        "\n",
        "shark_metadata = MetadataCatalog.get(\"shark_train\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a0169b819277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# for d in [\"train\", \"val\"]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#   # Register shark_dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#   # DatasetCatalog.register(\"shark_\" + d, lambda d=d: getSharkDicts(\"data/\" + d))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   # DatasetCatalog.register(\"shark_\" + d, lambda d=d: getSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/\" + d))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWxjUxTo4478",
        "colab_type": "text"
      },
      "source": [
        "Visualise to check this worked\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSiVnQEHzyNv",
        "colab_type": "code",
        "outputId": "ce4a60a5-e81e-4bb6-cf9e-d61cbf65a33f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "# dataset_dicts = getSharkDicts(\"/content/drive/My Drive/sharkdata/train\")\n",
        "dataset_dicts = getSharkTrainDicts() #getSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/train\")\n",
        "print(\"Done\")\n",
        "for dictionary in random.sample(dataset_dicts, 3):\n",
        "# for dictionary in dataset_dicts:\n",
        "  img = cv2.imread(dictionary[\"file_name\"])\n",
        "  visualizer = Visualizer(img[:, :, ::-1], metadata=shark_metadata, scale=0.2)\n",
        "  vis = visualizer.draw_dataset_dict(dictionary)\n",
        "  # Print the class ID\n",
        "  classID = ((dictionary[\"annotations\"])[0])[\"category_id\"]\n",
        "  print(ClassList[classID])\n",
        "\n",
        "  cv2_imshow(vis.get_image()[:, :, ::-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting shark dicts\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5b9f845f06b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSharkDicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/sharkdata/all_data/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for dictionary in dataset_dicts:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6cc20a403831>\u001b[0m in \u001b[0;36mgetSharkDicts\u001b[0;34m(dataDirectory)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Get image height and width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mimHeight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimWidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Construct annotation object (bounding box)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDw2f-vT7Gz",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeJn9u-nHTXe",
        "colab_type": "text"
      },
      "source": [
        "Training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmNsfY6U1KhE",
        "colab_type": "code",
        "outputId": "58c17498-5c74-4a33-a521-44aed932f5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "# COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n",
        "# COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\n",
        "# COCO-Detection/retinanet_R_50_FPN_1x.yaml\n",
        "# COCO-Detection/retinanet_R_50_FPN_3x.yaml\n",
        "# COCO-Detection/retinanet_R_101_FPN_3x.yaml\n",
        "modelLink = Detection/retinanet_R_101_FPN_3x.yaml\n",
        "\n",
        "# default configuration\n",
        "cfg = get_cfg()\n",
        "\n",
        "# get the pretrained retinanet model\n",
        "cfg.merge_from_file(model_zoo.get_config_file(modelLink))\n",
        "\n",
        "# list of the dataset names for training (registered in datasetcatalog (?))\n",
        "cfg.DATASETS.TRAIN = (\"shark_train\",)\n",
        "# list of the dataset names for testing (registered in datasetcatalog (?))\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "# number of data loading threads\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "# locate the pretrained weights\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(modelLink)  # Let training initialize from model zoo\n",
        "\n",
        "# number of images per batch\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "\n",
        "# learning rate\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "# cfg.SOLVER.BASE_LR = 0.0025  # pick a good LR\n",
        "\n",
        "# max iterations\n",
        "cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
        "# cfg.SOLVER.MAX_ITER = 500    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n",
        "\n",
        "# Minibatch size PER image - number of regions of interest (ROIs)\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "\n",
        "# Number of classes\n",
        "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n",
        "cfg.MODEL.RETINANET.NUM_CLASSES = len(SharkClassDictionary)  # only has one class (ballon)\n",
        "\n",
        "# directories (?)\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Get the default trainer:\n",
        "## 1) Create model, optimiser, scheduler, dataloader from the given config\n",
        "## 2) Load a checkpoint or cfg.MODEL.WEIGHTS if it exists\n",
        "## 3) Register a few common hooks (?)\n",
        "## This simplifies the standard model training workflow, so you don't have to write boilerplate code\n",
        "trainer = DefaultTrainer(cfg)\n",
        "# If true, and the last checkpoint exists, resume from it\n",
        "# If false, load a model specified by the config\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading config /content/detectron2_repo/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[01/28 22:12:57 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (6): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (7): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (8): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (9): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (10): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (11): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (12): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (13): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (14): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (15): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (16): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (17): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (18): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (19): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (20): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (21): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (22): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): RetinaNetHead(\n",
            "    (cls_subnet): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): ReLU()\n",
            "    )\n",
            "    (bbox_subnet): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU()\n",
            "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU()\n",
            "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (7): ReLU()\n",
            "    )\n",
            "    (cls_score): Conv2d(256, 2736, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): DefaultAnchorGenerator(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "Getting shark dicts\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwCWRFKFHhaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKDc2UBGT-_4",
        "colab_type": "text"
      },
      "source": [
        "# Inference and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2WXQFxiRJzW",
        "colab_type": "text"
      },
      "source": [
        "Inference and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN-M1rfpJ8UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference:\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
        "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.6\n",
        "# cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.25\n",
        "cfg.DATASETS.TEST = (\"shark_val\", )\n",
        "# cfg.DATASETS.TEST = (\"shark_train\", )\n",
        "# Create a simple end-to-end predictor with the given config\n",
        "## This predictor takes care of model loading and input preprocessing for you\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceUpQzjCR2t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualise:\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "# dataset_dicts = getSharkDicts(\"/content/drive/My Drive/sharkdata/train\")\n",
        "dataset_dicts = getSharkValDicts() #getSharkDicts(\"/content/drive/My Drive/sharkdata/all_data/val\")\n",
        "# for dictionary in random.sample(dataset_dicts, 12):\n",
        "for dictionary in random.sample(dataset_dicts, 12):\n",
        "  im = cv2.imread(dictionary[\"file_name\"])\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(im[:, :, ::-1],\n",
        "                  metadata=shark_metadata, \n",
        "                  scale=0.2, \n",
        "                #  instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
        "  )\n",
        "  v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "  # Print the class ID\n",
        "  classID = ((dictionary[\"annotations\"])[0])[\"category_id\"]\n",
        "  print(ClassList[classID])\n",
        "\n",
        "  img = v.get_image()[:, :, ::-1]\n",
        "  # cv2_imshow(img)\n",
        "  \n",
        "  filename = \"inferenceOutputs/\" + dictionary[\"id-\"] + dictionary[\"file_name\"]\n",
        "  cv2.imwrite(filename, img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzbFAHenR9In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AP\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"shark_val\", cfg, False, output_dir=\"./output/\")\n",
        "# evaluator = COCOEvaluator(\"shark_train\", cfg, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"shark_val\")\n",
        "# val_loader = build_detection_test_loader(cfg, \"shark_train\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)\n",
        "# another equivalent way is to use trainer.test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}