# -*- coding: utf-8 -*-
"""shark-initialseti.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oUGEC-RPu9DxnDLidVM2su46DVpjjnpV

# Installs and Imports
"""

print("Importing...")

# install dependencies:
# (use +cu100 because colab is on CUDA 10.0)
# !pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html 
# !pip install cython pyyaml==5.1
# !pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
import torch, torchvision
# torch.__version__
# !gcc --version
# opencv is pre-installed on colab

# install detectron2:
# !git clone https://github.com/facebookresearch/detectron2 detectron2_repo
# !pip install -e detectron2_repo

# You may need to restart your runtime prior to this, to let your installation take effect
# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import cv2
import random
# from google.colab.patches import cv2_imshow
import math

# import cv2
import os
import numpy as np
import json
from detectron2.structures import BoxMode

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

import argparse
import datetime
from collections import OrderedDict

from termcolor import colored, cprint
# print(colored('hello', 'red'), colored('world', 'green'))

import datetime
import logging

from detectron2.utils.events import EventWriter
from detectron2.utils.events import get_event_storage


from fvcore.nn.precise_bn import get_bn_modules, update_bn_stats


# from detectron2.evaluation import inference_on_dataset

print("Imports done")

dateTime = datetime.datetime.now()
dateTime = str(dateTime)

# from google.colab import drive
# drive.mount('/content/drive')

"""# Parser"""

parser = argparse.ArgumentParser(
    description="Train over small shark set",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument(
  "-lr",
  "--learning-rate",
  default=-1,
  type=float,
  help="Base learning rate used by the model"
)
parser.add_argument(
  "-m",
  "--model",
  default=-1,
  type=int,
  help="Model used"
)
parser.add_argument(
  "-i",
  "--max-iter",
  default=-1,
  type=int,
  help="Max number of iterations in training"
)
parser.add_argument(
  "-id",
  "--jobid",
  default=-1,
  type=int,
  help="The Slurm JOB ID - Not set by the user"
)
parser.add_argument(
  "-d",
  "--dataset",
  default="s",
  type=str,
  help="The dataset being used."
)

dataset_used = ""
if(parser.parse_args().dataset == "s"):
  dataset_used = "small"
  print("Dataset being used is the small dataset")
elif(parser.parse_args().dataset == "l"):
  dataset_used = "large"
  print("Dataset being used is the large dataset")
else:
  raise ValueError("Dataset arg provided \""+parser.parse_args().dataset+"\" is invalid")


"""# Directories"""

trainDirectory      = ""
valDirectory        = ""
imageDirectory      = ""
sourceJsonDirectory = ""
baseDirectory       = ""
baseOutputDirectory = ""

if(dataset_used == "small"):
  trainDirectory      = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/train/"
  valDirectory        = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/val/"
  imageDirectory      = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/images/"
  sourceJsonDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/data.json"
  baseDirectory       = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/small_set/photos/"
  baseOutputDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/outputs/small/"
if(dataset_used == "large"):
  trainDirectory      = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/train/"
  valDirectory        = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/val/"
  imageDirectory      = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/images/"
  sourceJsonDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/data.json"
  baseDirectory       = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/large_set/"
  baseOutputDirectory = "/mnt/storage/home/ja16475/sharks/detectron2/scratch/outputs/large/"


"""# Dataset

Construct Dictionary of SharkIDs to Classes
"""

# Registered in dataset loading
def getSharkDicts(trainVal):
  if(trainVal == "train"):
    # print("Getting shark train dicts")
    return getSharkTrainDicts()
  if(trainVal == "val"):
    # print("Getting shark val dicts")
    return getSharkValDicts()

# Called by getSharkDicts
def getSharkTrainDicts():
  return torch.load(trainDirectory+"sharkTrainDicts.pt")

# Called by getSharkDicts
def getSharkValDicts():
  return torch.load(valDirectory+"sharkValDicts.pt")

def getSharkClassDictionary():
  return torch.load(baseDirectory+"SharkClassDictionary.pt")

def getClassList():
  return torch.load(baseDirectory+"ClassList.pt")


SharkClassDictionary = getSharkClassDictionary()
ClassList = getClassList()

# Enter into dataset and metadata catalogues
from detectron2.data import DatasetCatalog, MetadataCatalog
for d in ["train", "val"]:
  # Register shark_train and shark_val
  DatasetCatalog.register("shark_" + d, lambda d=d: getSharkDicts(d))
  MetadataCatalog.get("shark_" + d).set(thing_classes=ClassList)

shark_metadata = MetadataCatalog.get("shark_train")

"""Visualise to check this worked"""

# dataset_dicts = getSharkDicts("/content/drive/My Drive/sharkdata/train")
# dataset_dicts = getSharkTrainDicts() #getSharkDicts("/content/drive/My Drive/sharkdata/all_data/train")
# dataset_dicts = getSharkValDicts()
# print("Done")
# for dictionary in random.sample(dataset_dicts, 12):
# # for dictionary in dataset_dicts:
#   scl = 0.2
#   img = cv2.imread(dictionary["file_name"])
#   visualizer = Visualizer(img[:, :, ::-1], metadata=shark_metadata, scale=scl)
#   vis = visualizer.draw_dataset_dict(dictionary)
#   # Print the class ID
#   classID = ((dictionary["annotations"])[0])["category_id"]
#   print()
#   print(ClassList[classID])
#   print(dictionary["file_name"])

#   # cv2_imshow(vis.get_image()[:, :, ::-1])

#   #crop
#   bbox = ((dictionary["annotations"])[0])["bbox"]
#   print(bbox) #"bbox": [xmin,ymin,xmax,ymax],
#   xmin,ymin,xmax,ymax = bbox
#   w = xmax-xmin
#   h = ymax-ymin
#   print(w,h)
#   # cropT = detectron2.data.transforms.CropTransform(xmin-5,ymin-5,w+5,h+5)
#   cropT = detectron2.data.transforms.CropTransform(math.floor((xmin-15)*scl),math.floor((ymin-15)*scl),math.floor((w+20)*scl),math.floor((h+20)*scl))
#   cropIm = cropT.apply_image(vis.get_image()[:, :, ::-1])
#   cv2_imshow(cropIm)

  # break


import time
from contextlib import contextmanager
from detectron2.evaluation.evaluator import DatasetEvaluators
# print(colored('hello', 'red'), colored('world', 'green'))

@contextmanager
def inference_context(model):
    """
    A context where the model is temporarily changed to eval mode,
    and restored to previous mode afterwards.

    Args:
        model: a torch Module
    """
    training_mode = model.training
    model.eval()
    yield
    model.train(training_mode)

def inference_on_dataset(model, data_loader, evaluator):
    """
    Run model on the data_loader and evaluate the metrics with evaluator.
    Also benchmark the inference speed of `model.forward` accurately.
    The model will be used in eval mode.

    Args:
        model (nn.Module): a module which accepts an object from
            `data_loader` and returns some outputs. It will be temporarily set to `eval` mode.

            If you wish to evaluate a model in `training` mode instead, you can
            wrap the given model and override its behavior of `.eval()` and `.train()`.
        data_loader: an iterable object with a length.
            The elements it generates will be the inputs to the model.
        evaluator (DatasetEvaluator): the evaluator to run. Use `None` if you only want
            to benchmark, but don't want to do any evaluation.

    Returns:
        The return value of `evaluator.evaluate()`
    """
    print(colored("Calculating inference...","green"))
    # num_devices = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1
    # logger = logging.getLogger(__name__)
    # logger.info("Start inference on {} images".format(len(data_loader)))

    total = len(data_loader)  # inference data loader must have a fixed length
    if evaluator is None:
        # create a no-op evaluator
        evaluator = DatasetEvaluators([])
    evaluator.reset()

    num_warmup = min(5, total - 1)
    # start_time = time.perf_counter()
    # total_compute_time = 0
    with inference_context(model), torch.no_grad():
        for idx, inputs in enumerate(data_loader):
            # if idx == num_warmup:
                # start_time = time.perf_counter()
                # total_compute_time = 0

            # start_compute_time = time.perf_counter()
            outputs = model(inputs)
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            # total_compute_time += time.perf_counter() - start_compute_time
            evaluator.process(inputs, outputs)

            # iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)
            # seconds_per_img = total_compute_time / iters_after_start
            # if idx >= num_warmup * 2 or seconds_per_img > 5:
                # total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start
                # eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))
                # log_every_n_seconds(
                #     logging.INFO,
                #     "Inference done {}/{}. {:.4f} s / img. ETA={}".format(
                #         idx + 1, total, seconds_per_img, str(eta)
                #     ),
                #     n=5,
                # )

    # Measure the time only for this worker (before the synchronization barrier)
    # total_time = time.perf_counter() - start_time
    # total_time_str = str(datetime.timedelta(seconds=total_time))
    # NOTE this format is parsed by grep
    # logger.info(
    #     "Total inference time: {} ({:.6f} s / img per device, on {} devices)".format(
    #         total_time_str, total_time / (total - num_warmup), num_devices
    #     )
    # )
    # total_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))
    # logger.info(
    #     "Total inference pure compute time: {} ({:.6f} s / img per device, on {} devices)".format(
    #         total_compute_time_str, total_compute_time / (total - num_warmup), num_devices
    #     )
    # )
    print(colored("Done calculating","green"))

    results = evaluator.evaluate()
    # An evaluator may return None when not in main process.
    # Replace it by an empty dict instead to make it easier for downstream code to handle
    if results is None:
        results = {}
    return results



############### Custom Classes ###############
from detectron2.evaluation import COCOEvaluator#, inference_on_dataset
from detectron2.evaluation import DatasetEvaluator
from detectron2.data import build_detection_test_loader

class TopKAccuracy(DatasetEvaluator):
  def __init__(self, k=5):
    self.k = k
  def reset(self):
    self.numberCorrect = 0
    self.totalNumber   = 0
  # I think this is a single batch, with the inputted images and outputted results
  def process(self, inputs, outputs):
    for input,output in zip(inputs,outputs):
      # Increment the total number no matter what
      self.totalNumber = self.totalNumber + 1

      # Get the true class ID
      # print(input)
      classID = input["classID"]
      trueSharkID = ClassList[classID]

      # Get the instances object from the outputs
      instances = output["instances"]
      
      # Get the predicted classes for this image
      classes = instances.get("pred_classes")
      # Convert classes to more useful sharkIDs
      predictedSharkIDs = []
      for c in classes:
        predictedSharkIDs.append(ClassList[c])

      # Get the list of scores for each prediction
      scores = instances.get("scores")
      scores = scores.cpu()
      scores = scores.numpy()
      
      # If there are no predicted scores for his input, skip iteration of the loop
      if(len(scores) == 0): continue

      # Zip up the predicted shark IDs and scores into a dictionary
      sharkIDScoreDict = dict(zip(predictedSharkIDs,scores))
      # Sort it into a list of descending order, in order of the value (the score)
      sortedSharkIDScoreList = sorted(sharkIDScoreDict.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)

      # sortedSharkIDScoreList is a list of tuples: [(sharkID,score),...]
      # sortedSharkIDScoreList[0] gives you the highest scoring tuple
      # (sortedSharkIDScoreList[0])[0] gives you the sharkID for the 0th tuple

      # Get the top K shark IDs
      # topKPredictedIDs = []
      for i in range(0,self.k):
        # If the list is shorter than the number of k's we want to look at, 
        # then break, no need to continue as there are no more predictions to consider
        if(i >= len(sortedSharkIDScoreList)): break
        # Extract ith tuple
        currentTuple = sortedSharkIDScoreList[i]
        # Get the shark ID
        currentPredID = currentTuple[0]
        currentScore = currentTuple[1]
        # Append this to the top K predictions
        # topKPredictedIDs.append(currentPredID)

        # We increase the rank of the correct prediction for each equivalence we find
        # So if there are many predictions with the same score to the correct prediction
        # we are "lowering" its rank to not consider it as much 
        # (where rank 0 is the highest)
        rank = -1

        # # If we're dealing with the true one
        # if(currentPredID == trueSharkID):
        #   # Go through all pairs
        #   for idx,scoreSharkIDPair in enumerate(sortedSharkIDScoreList):
        #     # If there is an equivalent score
        #     if(scoreSharkIDPair[1] == currentScore):
        #       rank = idx
        #       break

        # If the current predictedID we are considering is the trueSharkID
        if(currentPredID == trueSharkID):
          # Compare the correct prediction's score to all other scores
          for idx,scoreSharkIDPair in enumerate(sortedSharkIDScoreList):
            # If there is an equivalence in score
            if(scoreSharkIDPair[1] == currentScore):
              # If the rank hasn't been initialised, 
              # set it to the lowest index of this score
              if(rank == -1): 
                rank = idx + 1
              # If the rank has been set, increment
              else:
                # Increment the rank
                # Note, this will occur at least once as we compare it to itself
                rank = rank + 1
          # If the rank has exceed the number k we wanted to look at, don't count it
          if(rank <= self.k):
            # We increment, and then we don't care about the rest of the k's
            self.numberCorrect = self.numberCorrect + 1
            break

  # Return a dictionary of the final result
  def evaluate(self):
    # save self.count somewhere, or print it, or return it.
    accuracy = float(self.numberCorrect) / float(self.totalNumber)
    return {"total_num": self.totalNumber, "num_correct": self.numberCorrect, "accuracy": accuracy, "k": self.k}



"""# Dataset Mapping"""

############### Dataset Mapper ###############

# Building my own data loader
from detectron2.data import build_detection_train_loader
from detectron2.data import transforms as T
from detectron2.data import detection_utils as utils
import copy

def mapper(dataset_dict):
  # Implement a mapper, similar to the default DatasetMapper, but with your own customizations
  # Create a copy of the dataset dict
  dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
  # Read in the image
  image = utils.read_image(dataset_dict["file_name"], format="BGR")

  # cropping
  # Get boundingbox
  bbox = ((dataset_dict["annotations"])[0])["bbox"]
  # print(bbox)
  xmin,ymin,xmax,ymax = bbox
  w = xmax-xmin
  h = ymax-ymin

  # print(image.shape)

  cropT = T.CropTransform(xmin-15,ymin-15,w+50,h+50)
  image = cropT.apply_image(image)

  dataset_dict["height"] = h+15+50
  dataset_dict["width"] = w+15+50

  transforms = T.TransformList([cropT])

  # image, tfms = T.apply_transform_gens([T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), T.RandomFlip()], image)
  image, tfms = T.apply_transform_gens([T.RandomFlip()], image)
  transforms = transforms + tfms

  dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))

  classID = ((dataset_dict["annotations"])[0])["category_id"]

  annos = \
  [
      utils.transform_instance_annotations(obj, transforms, image.shape[:2])
      for obj in dataset_dict.pop("annotations")
      if obj.get("iscrowd", 0) == 0
  ]
  
  instances = utils.annotations_to_instances(annos, image.shape[:2])
  dataset_dict["instances"] = utils.filter_empty_instances(instances)

  dataset_dict["classID"] = classID

  return dataset_dict




from fvcore.common.file_io import PathManager
from PIL import Image

class DatasetMapper:
    """
    A callable which takes a dataset dict in Detectron2 Dataset format,
    and map it into a format used by the model.

    This is the default callable to be used to map your dataset dict into training data.
    You may need to follow it to implement your own one for customized logic.

    The callable currently does the following:

    1. Read the image from "file_name"
    2. Applies cropping/geometric transforms to the image and annotations
    3. Prepare data and annotations to Tensor and :class:`Instances`
    """

    def __init__(self, cfg, is_train=True):
        if cfg.INPUT.CROP.ENABLED and is_train:
            self.crop_gen = T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)
            logging.getLogger(__name__).info("CropGen used in training: " + str(self.crop_gen))
        else:
            self.crop_gen = None

        self.tfm_gens = utils.build_transform_gen(cfg, is_train)

        # fmt: off
        self.img_format     = cfg.INPUT.FORMAT
        self.mask_on        = cfg.MODEL.MASK_ON
        self.mask_format    = cfg.INPUT.MASK_FORMAT
        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON
        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS
        # fmt: on
        if self.keypoint_on and is_train:
            # Flip only makes sense in training
            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)
        else:
            self.keypoint_hflip_indices = None

        if self.load_proposals:
            self.min_box_side_len = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE
            self.proposal_topk = (
                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN
                if is_train
                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST
            )
        self.is_train = is_train

    def __call__(self, dataset_dict):
        """
        Args:
            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.

        Returns:
            dict: a format that builtin models in detectron2 accept
        """
        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
        # USER: Write your own image loading if it's not from a file
        image = utils.read_image(dataset_dict["file_name"], format=self.img_format)
        utils.check_image_size(dataset_dict, image)

        if "annotations" not in dataset_dict:
            image, transforms = T.apply_transform_gens(
                ([self.crop_gen] if self.crop_gen else []) + self.tfm_gens, image
            )
        else:
            # Crop around an instance if there are instances in the image.
            # USER: Remove if you don't use cropping
            if self.crop_gen:
                crop_tfm = utils.gen_crop_transform_with_instance(
                    self.crop_gen.get_crop_size(image.shape[:2]),
                    image.shape[:2],
                    np.random.choice(dataset_dict["annotations"]),
                )
                image = crop_tfm.apply_image(image)
            image, transforms = T.apply_transform_gens(self.tfm_gens, image)
            if self.crop_gen:
                transforms = crop_tfm + transforms

        image_shape = image.shape[:2]  # h, w

        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.

        # CROP
        bbox = ((dataset_dict["annotations"])[0])["bbox"]
        xmin,ymin,xmax,ymax = bbox
        w = xmax-xmin
        h = ymax-ymin

        cropT = T.CropTransform(xmin-15,ymin-15,w+50,h+50)
        image = cropT.apply_image(image)

        dataset_dict["height"] = h+15+50
        dataset_dict["width"] = w+15+50
        transforms = transforms + T.TransformList([cropT])
        image, tfms = T.apply_transform_gens([T.RandomFlip()], image)
        transforms = transforms + tfms

        dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1).copy()))

        # USER: Remove if you don't use pre-computed proposals.
        if self.load_proposals:
            utils.transform_proposals(
                dataset_dict, image_shape, transforms, self.min_box_side_len, self.proposal_topk
            )

        if not self.is_train:
            # USER: Modify this if you want to keep them for some reason.
            dataset_dict.pop("annotations", None)
            dataset_dict.pop("sem_seg_file_name", None)
            return dataset_dict

        if "annotations" in dataset_dict:
            # USER: Modify this if you want to keep them for some reason.
            for anno in dataset_dict["annotations"]:
                if not self.mask_on:
                    anno.pop("segmentation", None)
                if not self.keypoint_on:
                    anno.pop("keypoints", None)

            # USER: Implement additional transformations if you have other types of data
            annos = [
                utils.transform_instance_annotations(
                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices
                )
                for obj in dataset_dict.pop("annotations")
                if obj.get("iscrowd", 0) == 0
            ]
            instances = utils.annotations_to_instances(
                annos, image_shape, mask_format=self.mask_format
            )
            # Create a tight bounding box from masks, useful when image is cropped
            if self.crop_gen and instances.has("gt_masks"):
                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()
            dataset_dict["instances"] = utils.filter_empty_instances(instances)

        # USER: Remove if you don't do semantic/panoptic segmentation.
        if "sem_seg_file_name" in dataset_dict:
            with PathManager.open(dataset_dict.pop("sem_seg_file_name"), "rb") as f:
                sem_seg_gt = Image.open(f)
                sem_seg_gt = np.asarray(sem_seg_gt, dtype="uint8")
            sem_seg_gt = transforms.apply_segmentation(sem_seg_gt)
            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype("long"))
            dataset_dict["sem_seg"] = sem_seg_gt
        return dataset_dict







############### END Dataset Mapper ###############

def EvaluateTestTopKAccuracy(numK,isReturn=False):
  # with torch.no_grad():
  # Create evaluator object
  topKEvaluator = TopKAccuracy(numK)
  # Get the accuracy results
  val_loader = build_detection_test_loader(cfg, "shark_val", mapper=mapper)
  # val_loader = build_detection_test_loader(cfg, "shark_val", mapper=DatasetMapper(cfg,False))
  accuracy_results = inference_on_dataset(trainer.model, val_loader, topKEvaluator)

  if(isReturn):
    return accuracy_results
  else:
    # Extract results
    total_num   = str(accuracy_results["total_num"])
    num_correct = str(accuracy_results["num_correct"])
    top_k_acc   = str(round((accuracy_results["accuracy"]*100),2)) + "%"
    k           = str(accuracy_results["k"])



    # Create the string we're going to add to the text_file
    appendString = "\n________________________________________________________" \
                + "\nNumber correct: \t" + num_correct \
                + "\nTotal Number: \t\t" + total_num \
                + "\nTop " + str(k) + " Accuracy: \t" + top_k_acc \
                + "\n"

    # Append to the file
    text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
    text_file.write(appendString)
    text_file.close()

    # Print the file
    print(  "\nNumber correct: \t" + num_correct \
          + "\nTotal Number: \t\t" + total_num \
          + "\nTop " + str(k) + " Accuracy: \t" + top_k_acc \
          + "\n\n")

    result = OrderedDict()
    result["accuracy"]    = round(accuracy_results["accuracy"]*100,2)
    result["num_correct"] = accuracy_results["num_correct"]
    result["total_num"]   = accuracy_results["total_num"]
    result["k"]           = accuracy_results["k"]
    return result


def EvaluateTrainTopKAccuracy(numK, isReturn=False):
  # with torch.no_grad():
  # Create evaluator object
  topKEvaluator = TopKAccuracy(numK)

  train_loader = build_detection_test_loader(cfg, "shark_train", mapper=mapper)
  # train_loader = build_detection_test_loader(cfg, "shark_train", mapper=DatasetMapper(cfg,False))
  # Get the accuracy results
  accuracy_results = inference_on_dataset(trainer.model, train_loader, topKEvaluator)

  if(isReturn): 
    return accuracy_results
  else:
    # Extract results
    total_num   = str(accuracy_results["total_num"])
    num_correct = str(accuracy_results["num_correct"])
    top_k_acc   = str(round((accuracy_results["accuracy"]*100),2)) + "%"
    k           = str(accuracy_results["k"])

    # Create the string we're going to add to the text_file
    appendString = "\n________________________________________________________" \
                + "\nNumber correct: \t" + num_correct \
                + "\nTotal Number: \t\t" + total_num \
                + "\nTop " + str(k) + " Accuracy: \t" + top_k_acc \
                + "\n"

    # Append to the file
    text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
    text_file.write(appendString)
    text_file.close()

    # Print the file
    print(  "\nNumber correct: \t" + num_correct \
          + "\nTotal Number: \t\t" + total_num \
          + "\nTop " + str(k) + " Accuracy: \t" + top_k_acc \
          + "\n\n")

    # result = OrderedDict()
    # result["accuracy"]    = round(accuracy_results["accuracy"]*100,2)
    # result["num_correct"] = accuracy_results["num_correct"]
    # result["total_num"]   = accuracy_results["total_num"]
    # result["k"]           = accuracy_results["k"]
    # return result

'''
class MyCommonMetricPrinter(EventWriter):
    """
    Print **common** metrics to the terminal, including
    iteration time, ETA, memory, all losses, and the learning rate.

    To print something different, please implement a similar printer by yourself.
    """
    
    def __init__(self, max_iter):
      """
      Args:
          max_iter (int): the maximum number of iterations to train.
              Used to compute ETA.
      """
      

      # create logger
      self.logger = logging.getLogger(__name__)
      self.logger.setLevel(logging.DEBUG)

      # create console handler and set level to debug
      ch = logging.StreamHandler()
      ch.setLevel(logging.DEBUG)

      # create formatter
      formatter = logging.Formatter('[%(asctime)s - %(name)s]: %(message)s')

      # add formatter to ch
      ch.setFormatter(formatter)

      # add ch to logger
      self.logger.addHandler(ch)

      # self.logger.info('info message')

      # self.logger = logging.getLogger("hello")
      # self.logger = logging.getLogger(__name__)
      self._max_iter = max_iter


    def write(self):
        # print("called")

        # TRY JUST LOGGING ANYTHING RIGHT HERE???
        # self.logger.debug('debug message')
        # I think the only problem right now is that for some reason the logging isn't working
        # It's getting called...
        # If I really want to, I could just print the accuracy 
        #   -> note that none of this is going to the tensorboard (or json), so need to look into how to log that

        storage = get_event_storage()
        iteration = storage.iter

        data_time, time = None, None
        eta_string = "N/A"
        try:
            data_time = storage.history("data_time").avg(20)
            time = storage.history("time").global_avg()
            eta_seconds = storage.history("time").median(1000) * (self._max_iter - iteration)
            storage.put_scalar("eta_seconds", eta_seconds, smoothing_hint=False)
            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
            # accuracy_string = "acc"
        except KeyError:  # they may not exist in the first few iterations (due to warmup)
            pass

        try:
            lr = "{:.6f}".format(storage.history("lr").latest())
        except KeyError:
            lr = "N/A"

        if torch.cuda.is_available():
            max_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0
        else:
            max_mem_mb = None

        lossesString ="  ".join(["{}: {:.3f}".format(k, v.median(20))
                          for k, v in storage.histories().items()
                          if "loss" in k])
                         
        timeString = "time: {:.4f}".format(time) if time is not None else ""
        data_time_string = "data_time: {:.4f}".format(data_time) if data_time is not None else ""
        memoryString = "max_mem: {:.0f}M".format(max_mem_mb) if max_mem_mb is not None else ""

        # Compute the accuracy
        trainResult = EvaluateTrainTopKAccuracy(1,isReturn=True)
        train_accuracy = round((trainResult["accuracy"]*100),2)
        train_accuracy_string = str(train_accuracy)+"%"
        storage.put_scalar("accuracy_train",train_accuracy)

        testResult = EvaluateTopKAccuracy(1,isReturn=True)
        test_accuracy = round( (testResult["accuracy"]*100) , 2 )
        test_accuracy_string = str(test_accuracy)+"%"
        storage.put_scalar("accuracy_test",test_accuracy)


        
        # testing
        # accuracy_string = "temp"
        # storage.put_scalar("accuracy",0)

        # print(f"{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}")

        logRedString = "eta: "+eta_string+"  iter: "+str(iteration)
        # logString = "eta: "+eta_string+"  iter: "+str(iteration) +"  " +str(lossesString)+"  accuracy: "+accuracy_string+"  "+timeString+"  "+data_time_string+"  "+"lr: "+str(lr)+"  "+memoryString
        logString = "  " +str(lossesString)+"  Train Accuracy: "+train_accuracy_string+"  Test Accuracy: "+test_accuracy_string+"  "+timeString+"  "+data_time_string+"  "+"lr: "+str(lr)+"  "+memoryString
        print(colored(logRedString,"red")+logString)
'''

class TensorboardAndLogWriter(EventWriter):
  """
  Write all scalars to a tensorboard file.
  """

  def __init__(self, max_iter: int, log_dir: str, window_size: int = 20, **kwargs):
    """
    Args:
        log_dir (str): the directory to save the output events
        window_size (int): the scalars will be median-smoothed by this window size

        kwargs: other arguments passed to `torch.utils.tensorboard.SummaryWriter(...)`
    """
    self._window_size = window_size
    from torch.utils.tensorboard import SummaryWriter

    self._writer = SummaryWriter(log_dir, **kwargs)

    self._max_iter = max_iter


  def write(self):
    # Get the storage
    storage = get_event_storage()

    # if(storage )
    # Evaluate accuracy
    # result_train = EvaluateTrainTopKAccuracy(1,isReturn=True)
    # result_test  = EvaluateTopKAccuracy(1,isReturn=True)
    result_train = {}
    result_train["accuracy"] = -1
    result_test = {}
    result_test["accuracy"] = -1

    # Add accuracy scalar
    # print(result_train["accuracy"])
    # print(result_train["accuracy"]*100)
    # print(round(result_train["accuracy"]*100,2))
    accuracy_train = round((result_train["accuracy"]*100),2)
    accuracy_test  = round((result_test["accuracy"]*100 ),2)
    
    storage.put_scalar("accuracy_train",accuracy_train,smoothing_hint=False)
    storage.put_scalar("accuracy_test",accuracy_test,smoothing_hint=False)
    # accuracy_test = -1

    # self._writer.add_scalar("accuracy_train", accuracy_train, storage.iter)
    # self._writer.add_scalar("accuracy_test", accuracy_test, storage.iter)



    for k, v in storage.latest_with_smoothing_hint(self._window_size).items():
      # if(k != "accuracy")
      self._writer.add_scalar(k, v, storage.iter)
      # print(k,v,storage.iter)

    if len(storage.vis_data) >= 1:
      for img_name, img, step_num in storage.vis_data:
        self._writer.add_image(img_name, img, step_num)
      storage.clear_images()


    # Below here: logging
    # storage = get_event_storage()
    iteration = storage.iter

    data_time, time = None, None
    eta_string = "N/A"
    try:
        data_time = storage.history("data_time").avg(20)
        time = storage.history("time").global_avg()
        eta_seconds = storage.history("time").median(1000) * (self._max_iter - iteration)
        storage.put_scalar("eta_seconds", eta_seconds, smoothing_hint=False)
        eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
        # accuracy_string = "acc"
    except KeyError:  # they may not exist in the first few iterations (due to warmup)
        pass

    try:
        lr = "{:.6f}".format(storage.history("lr").latest())
    except KeyError:
        lr = "N/A"

    if torch.cuda.is_available():
        max_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0
    else:
        max_mem_mb = None

    lossesString ="  ".join(["{}: {:.3f}".format(k, v.median(20))
                      for k, v in storage.histories().items()
                      if "loss" in k])
                      
    timeString = "time: {:.4f}".format(time) if time is not None else ""
    data_time_string = "data_time: {:.4f}".format(data_time) if data_time is not None else ""
    memoryString = "max_mem: {:.0f}M".format(max_mem_mb) if max_mem_mb is not None else ""

    # Create the accuracy string
    train_accuracy_string = str(accuracy_train)
    test_accuracy_string  = str(accuracy_test)
    
    # testing
    # accuracy_string = "temp"
    # storage.put_scalar("accuracy",0)

    logRedString = "eta: "+eta_string+"  iter: "+str(iteration)
    # logString = "eta: "+eta_string+"  iter: "+str(iteration) +"  " +str(lossesString)+"  accuracy: "+accuracy_string+"  "+timeString+"  "+data_time_string+"  "+"lr: "+str(lr)+"  "+memoryString
    logString = "  " +str(lossesString)+"  train accuracy: "+train_accuracy_string +"  test accuracy: "+test_accuracy_string +"  "+timeString+"  "+data_time_string+"  "+"lr: "+str(lr)+"  "+memoryString
    print(colored(logRedString,"red")+logString)


  def close(self):
    if hasattr(self, "_writer"):  # doesn't exist when the code fails at import
      self._writer.close()



from detectron2.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter
from detectron2.engine import DefaultTrainer
import logging
from detectron2.utils import comm
from detectron2.engine import hooks

class Trainer(DefaultTrainer):
  # @classmethod
  # def build_evaluator(cls, cfg, dataset_name):
  #     output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
  #     evaluators = [COCOEvaluator(dataset_name, cfg, True, output_folder)]
  #     if cfg.MODEL.DENSEPOSE_ON:
  #         evaluators.append(DensePoseCOCOEvaluator(dataset_name, True, output_folder))
  #     return DatasetEvaluators(evaluators)

  # def __init__(self,cfg):
    # super().__init__(self,cfg)

  @classmethod
  def build_test_loader(cls, cfg, dataset_name):
    return build_detection_test_loader(cfg, dataset_name, mapper=mapper)
    # return build_detection_test_loader(cfg, dataset_name, mapper=DatasetMapper(cfg,False))

  @classmethod
  def build_train_loader(cls, cfg):
    return build_detection_train_loader(cfg, mapper=mapper)
    # return build_detection_train_loader(cfg, mapper=DatasetMapper(cfg,True))

  # @classmethod
  def build_writers(self):
    """
    Build a list of writers to be used. By default it contains
    writers that write metrics to the screen,
    a json file, and a tensorboard event file respectively.
    If you'd like a different list of writers, you can overwrite it in
    your trainer.

    Returns:
        list[EventWriter]: a list of :class:`EventWriter` objects.

    It is now implemented by:

    .. code-block:: python

        return [
            CommonMetricPrinter(self.max_iter),
            JSONWriter(os.path.join(self.cfg.OUTPUT_DIR, "metrics.json")),
            TensorboardXWriter(self.cfg.OUTPUT_DIR),
        ]

    """
    # Assume the default print/log frequency.
    return [
        # It may not always print what you want to see, since it prints "common" metrics only.
        # CommonMetricPrinter(self.max_iter),
        # MyCommonMetricPrinter(self.max_iter),
        JSONWriter(os.path.join(self.cfg.OUTPUT_DIR, "metrics.json")),
        # TensorboardXWriter(self.cfg.OUTPUT_DIR),
        # MyTensorboardXWriter(self.cfg.OUTPUT_DIR),
        TensorboardAndLogWriter(self.max_iter,self.cfg.OUTPUT_DIR+"/tensorboard"),
    ]

  # @classmethod
  def build_hooks(self):
    """
    Build a list of default hooks, including timing, evaluation,
    checkpointing, lr scheduling, precise BN, writing events.

    Returns:
        list[HookBase]:
    """
    cfg = self.cfg.clone()
    cfg.defrost()
    cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN

    ret = \
    [
      hooks.IterationTimer(),
      hooks.LRScheduler(self.optimizer, self.scheduler),
      hooks.PreciseBN(
          # Run at the same freq as (but before) evaluation.
          cfg.TEST.EVAL_PERIOD,
          self.model,
          # Build a new data loader to not affect training
          self.build_train_loader(cfg),
          cfg.TEST.PRECISE_BN.NUM_ITER,
      )
      if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)
      else None,
    ]

    # Do PreciseBN before checkpointer, because it updates the model and need to
    # be saved by checkpointer.
    # This is not always the best: if checkpointing has a different frequency,
    # some checkpoints may have more precise statistics than others.
    if comm.is_main_process():
      ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))

    def test_and_save_results():
      self._last_eval_results = self.test(self.cfg, self.model)
      return self._last_eval_results

    # Do evaluation after checkpointer, because then if it fails,
    # we can use the saved checkpoint to debug.
    ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))

    if comm.is_main_process():
      numberOfSamples = 25
      step = -1
      if(self.max_iter <= numberOfSamples):
        # Eg, maxiter = 20, so step = 20/2 = 10, take a sample every 10
        step = int(round(float(self.max_iter)/float(2),2))
      else:
        # Eg 10000/20 = 500, so will take a sample every 500 iterations
        step = float(self.max_iter)/float(numberOfSamples)
        step = int(round(step,0))
        if(step < 1): step = 1

      # print("!!!!!!!!!!!!!!STEPS: ", step)
      # ret.append(hooks.PeriodicWriter(self.build_writers()))
      # run writers in the end, so that evaluation metrics are written
      ret.append(hooks.PeriodicWriter(self.build_writers(),period=step))
      # ret.append(hooks.PeriodicWriter(self.build_writers(),period=(self.max_iter-1)))
    return ret

  # @classmethod
  # def build_model(cls, cfg):
  #     """
  #     Returns:
  #         torch.nn.Module:

  #     It now calls :func:`detectron2.modeling.build_model`.
  #     Overwrite it if you'd like a different model.
  #     """
  #     model = build_model(cfg)
  #     logger = logging.getLogger(__name__)
  #     logger.info("Model:\n{}".format(model))
  #     return model

############### END Custom Classes ###############




############### Custom Trainer ###############
# from detectron2.engine import DefaultTrainer

# class Trainer(DefaultTrainer):
#     # @classmethod
#     # def build_evaluator(cls, cfg, dataset_name):
#     #     output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
#     #     evaluators = [COCOEvaluator(dataset_name, cfg, True, output_folder)]
#     #     if cfg.MODEL.DENSEPOSE_ON:
#     #         evaluators.append(DensePoseCOCOEvaluator(dataset_name, True, output_folder))
#     #     return DatasetEvaluators(evaluators)

#     @classmethod
#     def build_test_loader(cls, cfg, dataset_name):
#         return build_detection_test_loader(cfg, dataset_name, mapper=mapper)

#     @classmethod
#     def build_train_loader(cls, cfg):
#         return build_detection_train_loader(cfg, mapper=mapper)

############### END Custom Trainer ###############


############### Training Configuration ###############
"""# Training"""

from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
import shutil



# 0: COCO-Detection/retinanet_R_50_FPN_1x.yaml
# 1: COCO-Detection/retinanet_R_50_FPN_3x.yaml
# 2: COCO-Detection/retinanet_R_101_FPN_3x.yaml
# 3: COCO-Detection/faster_rcnn_R_50_C4_1x.yaml
# 4: COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml

# modelLink = "COCO-Detection/retinanet_R_50_FPN_1x.yaml"
modelLink = ""
modelOutputFolderName = ""
if(parser.parse_args().model == 0):
  modelLink = "COCO-Detection/retinanet_R_50_FPN_1x.yaml"
  modelOutputFolderName = "retinanet_R_50_FPN_1x"
elif(parser.parse_args().model == 1):
  modelLink = "COCO-Detection/retinanet_R_50_FPN_3x.yaml"
  modelOutputFolderName = "retinanet_R_50_FPN_3x"
elif(parser.parse_args().model == 2):
  modelLink = "COCO-Detection/retinanet_R_101_FPN_3x.yaml"
  modelOutputFolderName = "retinanet_R_101_FPN_3x"
elif(parser.parse_args().model == 3):
  modelLink = "COCO-Detection/faster_rcnn_R_50_C4_1x.yaml"
  modelOutputFolderName = "faster_rcnn_R_50_C4_1x"
elif(parser.parse_args().model == 4):
  modelLink = "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
  modelOutputFolderName = "mask_rcnn_R_50_FPN_3x"
else:
  modelLink = "COCO-Detection/retinanet_R_50_FPN_1x.yaml"
  modelOutputFolderName = "retinanet_R_50_FPN_1x"


# default configuration
cfg = get_cfg()

# get the pretrained retinanet model
cfg.merge_from_file(model_zoo.get_config_file(modelLink))
# cfg.merge_from_file(model_zoo.get(modelLink,trained=False))#?

# list of the dataset names for training (registered in datasetcatalog)
# cfg.DATASETS.TRAIN = ("shark_train",)
cfg.DATASETS.TRAIN = ("shark_val",)
# list of the dataset names for testing (registered in datasetcatalog)
cfg.DATASETS.TEST = ()
# cfg.DATASETS.TEST = ("shark_val", )


##random cropping
# cfg.INPUT.CROP({"ENABLED": False})
# cfg.INPUT.CROP.ENABLED = True
##

# cfg.INPUT.MIN_SIZE_TRAIN = 200
# cfg.INPUT.MIN_SIZE_TEST  = 200
# Size of the smallest side of the image during training
# _C.INPUT.MIN_SIZE_TRAIN = (800,)
# # Sample size of smallest side by choice or random selection from range give by
# # INPUT.MIN_SIZE_TRAIN
# _C.INPUT.MIN_SIZE_TRAIN_SAMPLING = "choice"
# # Maximum size of the side of the image during training
# _C.INPUT.MAX_SIZE_TRAIN = 1333
# # Size of the smallest side of the image during testing. Set to zero to disable resize in testing.
# _C.INPUT.MIN_SIZE_TEST = 800
# # Maximum size of the side of the image during testing
# _C.INPUT.MAX_SIZE_TEST = 1333

# number of data loading threads
cfg.DATALOADER.NUM_WORKERS = 2

# locate the pretrained weights
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(modelLink)  # Let training initialize from model zoo

# number of images per batch
cfg.SOLVER.IMS_PER_BATCH = 2

# learning rate
# cfg.SOLVER.BASE_LR = 0.0000025  # pick a good LR
if(parser.parse_args().learning_rate == -1):
  cfg.SOLVER.BASE_LR = 0.005  # pick a good LR
else:
  cfg.SOLVER.BASE_LR = parser.parse_args().learning_rate

# max iterations
if(parser.parse_args().max_iter == -1):
  cfg.SOLVER.MAX_ITER = 10000
else:
  cfg.SOLVER.MAX_ITER = parser.parse_args().max_iter


# Minibatch size PER image - number of regions of interest (ROIs)
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 #lower is faster, default: 512

# Number of classes
cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(SharkClassDictionary)  # only has one class (ballon)
cfg.MODEL.RETINANET.NUM_CLASSES = len(SharkClassDictionary)  # only has one class (ballon)

# Creating and setting output folder path
# def CreateOutputFolder(counter):
#   # Convert counter to a string
#   ctrString = str(counter)
  
#   # Append 0s
#   if(counter < 100):
#     ctrString = "0" + ctrString
#     if(counter < 10):
#       ctrString = "0" + ctrString

#   # Create the folder and path names
#   foldername = "output"+ctrString
#   path = baseOutputDirectory + modelOutputFolderName + "/" + foldername

#   # Get list of all folders in output folder
#   currentOutFolders = os.listdir(baseOutputDirectory)
#   isFolder = False
#   # If the path exists, then note that it exists
#   if(os.path.isdir(path)):
#     isFolder = True
#   # If the list includes the correct form
#   for fldr in currentOutFolders:
#     # If in form outputXXX_<jobid>
#     if(len(fldr) >= 9):
#       if(fldr[:9] == path): 
#         isFolder = True

#   # if(os.path.isdir(path)):
#   # If it exists, recurse to the next number
#   if(isFolder):
#     nextNumber = counter + 1
#     # CreateOutputFolder(nextNumber)
#   else:
#     # Create the directory
#     jbName = str(parser.parse_args().jobid)
#     path = path + "_" + jbName
#     os.makedirs(path, exist_ok=True)
#     cfg.OUTPUT_DIR = path

def CreateOutputFolder():
  jbName = str(parser.parse_args().jobid)
  foldername = "output_"+jbName
  path = baseOutputDirectory + modelOutputFolderName + "/" + foldername
  os.makedirs(path, exist_ok=True)
  cfg.OUTPUT_DIR = path

# Create a folder output0 etc.
# CreateOutputFolder(0)
CreateOutputFolder()


# Define the we use trainer:
## 1) Create model, optimiser, scheduler, dataloader from the given config
## 2) Load a checkpoint or cfg.MODEL.WEIGHTS if it exists
## 3) Register a few common hooks (?)
## This simplifies the standard model training workflow, so you don't have to write boilerplate code
# trainer = DefaultTrainer(cfg)
trainer = Trainer(cfg)

print("Outputting to: ",cfg.OUTPUT_DIR)
print("Model being used: ",modelLink)
print("Model index: ",parser.parse_args().model)
print("Learning rate: ",cfg.SOLVER.BASE_LR)
print("Max iterations: ",cfg.SOLVER.MAX_ITER)
print("Number of classes: ",cfg.MODEL.RETINANET.NUM_CLASSES)
jbName = str(parser.parse_args().jobid)
OutputString = "\nDate time: \t"    + dateTime \
             + "\nJobname: \t" + jbName \
             + "\n________________________________________________________" \
             + "\nModel being used: \t" + modelLink \
             + "\nModel index: \t" + str(parser.parse_args().model) \
             + "\nLearning rate: \t\t"     + str(cfg.SOLVER.BASE_LR) \
             + "\nMax iterations: \t"    + str(cfg.SOLVER.MAX_ITER) \
             + "\nNumber of classes: \t" + str(cfg.MODEL.RETINANET.NUM_CLASSES) \
             + "\n________________________________________________________" \
             + "\n"

text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "w")
text_file.write(OutputString)
text_file.close()
# torch.save(OutputString,cfg.OUTPUT_DIR+"/parameters-information.txt")

############### END Training Configuration ###############

############### Training ###############
# If true, and the last checkpoint exists, resume from it
# If false, load a model specified by the config
trainer.resume_or_load(resume=False)
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# !kill 1825
# %load_ext tensorboard
# %tensorboard --logdir output
############### END Training ###############

"""# Inference and Evaluation"""

# Inference:
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set the testing threshold for this model
cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.7
cfg.DATASETS.TEST = ("shark_val", )
# cfg.DATASETS.TEST = ("shark_train", )
# Create a simple end-to-end predictor with the given config
## This predictor takes care of model loading and input preprocessing for you
try:
  predictor = DefaultPredictor(cfg)
except AssertionError:
  print("Checkpoint not found, model not found")
  ### Move the Slurm file ###
  # Get the jobname
  jobName = str(parser.parse_args().jobid)
  print("Moving ",jobName)
  # Create the file name
  filename = "slurm-"+jobName+".out"
  # Copy the file
  shutil.copy("/mnt/storage/home/ja16475/sharks/detectron2/"+filename, cfg.OUTPUT_DIR+"/"+filename)
  # Delete the original 
  os.remove("/mnt/storage/home/ja16475/sharks/detectron2/"+filename)
  raise AssertionError("model_final.pth not found! It's likely that training somehow failed.")

'''
# Visualise:
# from detectron2.utils.visualizer import ColorMode
# dataset_dicts = getSharkTrainDicts()
dataset_dicts = getSharkValDicts()
for dictionary in random.sample(dataset_dicts, 12):
  im = cv2.imread(dictionary["file_name"])
  outputs = predictor(im)
  v = Visualizer(im[:, :, ::-1],
                  metadata=shark_metadata, 
                  scale=0.1,
                #  instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
  )

  classID = ((dictionary["annotations"])[0])["category_id"]
  sharkID = ClassList[classID]

  instances = outputs["instances"]
  classes = instances.get("pred_classes")
  sharkIDs = []
  for c in classes:
    sharkIDs.append(ClassList[c])
  # print(sharkIDs)
  scoresRaw = instances.get("scores")
  scores = []
  for s in scoresRaw:
    # sStr = str(s.item())
    # sStr = sStr[:4]
    s = s.item()
    s = round(s,2)
    scores.append(s)
  out = dict(zip(sharkIDs,scores))
  print(out)

  highestScoringClass = ""
  highestScore = 0.0
  for s in out:
    floatS = float(out[s])
    if(floatS > highestScore): 
      highestScore = floatS
      highestScoringClass = out

  if(sharkID in out):
    if(highestScoringClass == sharkID):
      print("Correct prediction, and highest predicted: ", sharkID, out[sharkID])
    else:
      print("Correct prediction: ", sharkID, out[sharkID])
  else:
    print("No prediction: ", sharkID, "0.00")

  v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
  img = v.get_image()[:, :, ::-1]
  # cv2_imshow(img)
  # os.makedirs(baseDirectory + "outputs/", exist_ok=True)
  # if(not os.path.isdir(cfg.OUTPUT_DIR + "/predictions"))

  initialPath = os.getcwd()
  os.makedirs(cfg.OUTPUT_DIR + "/predictions", exist_ok=True)
  os.chdir(cfg.OUTPUT_DIR + "/predictions")
  imageFilename = dictionary["file_name"] + "_" + sharkID + ".jpg"
  cv2.imwrite(imageFilename, img)
  os.chdir(initialPath)
  # filename = cfg.OUTPUT_DIR + "/predictions/" + dictionary["file_name"] + "_" + sharkID + ".jpg"


'''




############### Evaluation ###############


# Evaluation
from detectron2.evaluation import COCOEvaluator#, inference_on_dataset
from detectron2.evaluation import DatasetEvaluator
from detectron2.data import build_detection_test_loader

evaluationDict = OrderedDict()

# The loader for the test data (applies various transformations if we so choose)
val_loader = build_detection_test_loader(cfg, "shark_val", mapper=mapper)
# val_loader = build_detection_test_loader(cfg, "shark_val", mapper=DatasetMapper(cfg,False))

def COCOEvaluation():
  # Get the coco evaluator
  cocoEvaluator = COCOEvaluator("shark_val", cfg, False, output_dir=cfg.OUTPUT_DIR+"/")

  # Run the model on the data_loader and evaluate the metrics evaluator
  # Also benchmarks the inference speed of model.forward accurately
  cocoOutput = inference_on_dataset(trainer.model, val_loader, cocoEvaluator)

  # "bbox": ["AP", "AP50", "AP75", "APs", "APm", "APl"]
  cocoBbox = cocoOutput["bbox"]
  mAP   = round(cocoBbox["AP"],2)
  mAP50 = round(cocoBbox["AP50"],2)
  mAP75 = round(cocoBbox["AP75"],2)
  mAPs  = round(cocoBbox["APs"],2)
  mAPm  = round(cocoBbox["APm"],2)
  mAPl  = round(cocoBbox["APl"],2)
  
  copycocoB = copy.deepcopy(cocoBbox)
  copycocoB.pop("AP")
  copycocoB.pop("AP50")
  copycocoB.pop("AP75")
  copycocoB.pop("APs")
  copycocoB.pop("APm")
  copycocoB.pop("APl")

  numAPClasses = 0
  averageScore = 0
  for APClass in copycocoB:
    numAPClasses = numAPClasses + 1
    if(not math.isnan(copycocoB[APClass])):
      averageScore = averageScore + copycocoB[APClass]

  averageScore = float(averageScore) / float(numAPClasses)
  # averageScore = str(averageScore)

  # print(cocoBbox["AP"])
  # another equivalent way is to use trainer.test

  # Create the string we're going to add to the text_file
  appendString = "\n________________________________________________________" \
                + "\nAverage Precision COCO: \t" + str(mAP) \
                + "\nAverage Precision 50  : \t" + str(mAP50) \
                + "\nAverage Precision 75  : \t" + str(mAP75) \
                + "\nAverage Precision Small : \t" + str(mAPs) \
                + "\nAverage Precision Medium: \t" + str(mAPm) \
                + "\nAverage Precision Large : \t" + str(mAPl) \
                + "\nMean Average Precision: \t" + str(averageScore) \
                + "\n"

  # Append to the file
  text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
  text_file.write(appendString)
  text_file.close()

  resultDict = OrderedDict()
  resultDict["AP"]   = mAP
  resultDict["AP50"] = mAP50
  resultDict["AP75"] = mAP75
  resultDict["APs"]  = mAPs
  resultDict["APm"]  = mAPm
  resultDict["APl"]  = mAPl
  resultDict["ClassAveAP"] = averageScore
  resultDict["PerClassAP"] = copycocoB
  return resultDict

cocoResults = COCOEvaluation()

parameterDict = OrderedDict()
parameterDict["jobid"] = parser.parse_args().jobid
parameterDict["output_directory"] = cfg.OUTPUT_DIR
parameterDict["model"] = modelOutputFolderName
parameterDict["model_index"] = parser.parse_args().model
parameterDict["lr"] = cfg.SOLVER.BASE_LR
parameterDict["max_iter"] = cfg.SOLVER.MAX_ITER
parameterDict["batch_size_per_image"] = cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE
parameterDict["num_classes"] = cfg.MODEL.RETINANET.NUM_CLASSES
parameterDict["transforms"] = "Crop to bounding box"

evaluationDict["params"] = parameterDict


evaluationDict["coco"] = cocoResults


'''
class Top1Accuracy(DatasetEvaluator):
  def reset(self):
    self.numberCorrect = 0
    self.totalNumber   = 0
  # I think this is a single batch, with the inputted images and outputted results
  def process(self, inputs, outputs):
    for input,output in zip(inputs,outputs):
      # Finally, increment the total number no matter what
      self.totalNumber = self.totalNumber + 1

      # prediction = 
      # self.count += len(output["instances"])
      # print(input)
      classID = input["classID"]
      trueSharkID = ClassList[classID]

      # Get the instance objects from the outputs
      instances = output["instances"]
      # Get the classes
      classes = instances.get("pred_classes")
      # Convert classes to more useful sharkIDs
      sharkIDs = []
      for c in classes:
        sharkIDs.append(ClassList[c])
      # Get the list of scores
      scores = instances.get("scores")
      scores = scores.cpu()
      scores = scores.numpy()
      if(len(scores) == 0): break
      # Get the highest scores's indexes (argmax will get a list of indexes for the max score)
      highestScore = np.max(scores)
      highScoreIndexes = []
      for idx,scr in enumerate(scores):
        if(scr == highestScore): 
          highScoreIndexes.append(idx)
      # highScoreIndexes = np.argmax(scores)
      # Get the sharkIDs that are the highest predicted
      highScoreSharkIDs = []
      for index in highScoreIndexes:
        highScoreSharkIDs.append(sharkIDs[index])
      # Check if the true ID is in the list of high scoring sharks
      if(trueSharkID in highScoreSharkIDs):
        # If it is in there, increment
        self.numberCorrect = self.numberCorrect + 1
      # Else: 
        # Don't do anything
  # Return a dictionary of the final result
  def evaluate(self):
    # save self.count somewhere, or print it, or return it.
    accuracy = float(self.numberCorrect) / float(self.totalNumber)
    return {"total_num": self.totalNumber, "num_correct": self.numberCorrect, "accuracy": accuracy}


myEvaluator = Top1Accuracy()
# This first calls reset, then process, and finally evaluate
# Reset just sets everything up
# Process takes in an input and output and processes them
# Evaluate should simply return all the results
accuracy_results = inference_on_dataset(trainer.model, val_loader, myEvaluator)

total_num   = str(accuracy_results["total_num"])
num_correct = str(accuracy_results["num_correct"])
top_1_acc   = str(accuracy_results["accuracy"])
print(  "\nNumber Correct: \t" + num_correct \
      + "\nTotal Number: \t\t" + total_num \
      + "\nTop 1 Accuracy: \t" + top_1_acc \
      + "\n")
# accuracy_string = total_num + 

appendString = "\n________________________________________________________" \
             + "\nNumber correct: \t" + num_correct \
             + "\nTotal Number: \t\t" + total_num \
             + "\nTop 1 Accuracy: \t" + top_1_acc \
             + "\n"

text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
text_file.write(appendString)
text_file.close()
'''

'''
# k = 1 should be equivalent
class TopKAccuracy(DatasetEvaluator):
  def __init__(self, k=5):
    self.k = k
  def reset(self):
    self.numberCorrect = 0
    self.totalNumber   = 0
  # I think this is a single batch, with the inputted images and outputted results
  def process(self, inputs, outputs):
    for input,output in zip(inputs,outputs):
      # Increment the total number no matter what
      self.totalNumber = self.totalNumber + 1

      # Get the true class ID
      classID = input["classID"]
      trueSharkID = ClassList[classID]

      # Get the instances object from the outputs
      instances = output["instances"]
      
      # Get the predicted classes for this image
      classes = instances.get("pred_classes")
      # Convert classes to more useful sharkIDs
      predictedSharkIDs = []
      for c in classes:
        predictedSharkIDs.append(ClassList[c])

      # Get the list of scores for each prediction
      scores = instances.get("scores")
      scores = scores.cpu()
      scores = scores.numpy()
      
      # If there are no predicted scores for his input, skip iteration of the loop
      if(len(scores) == 0): continue

      # Zip up the predicted shark IDs and scores into a dictionary
      sharkIDScoreDict = dict(zip(predictedSharkIDs,scores))
      # Sort it into a list of descending order, in order of the value (the score)
      sortedSharkIDScoreList = sorted(sharkIDScoreDict.items(), key = lambda kv:(kv[1], kv[0]), reverse=True)

      # sortedSharkIDScoreList is a list of tuples: [(sharkID,score),...]
      # sortedSharkIDScoreList[0] gives you the highest scoring tuple
      # (sortedSharkIDScoreList[0])[0] gives you the sharkID for the 0th tuple

      # Get the top K shark IDs
      # topKPredictedIDs = []
      for i in range(0,self.k):
        # If the list is shorter than the number of k's we want to look at, 
        # then break, no need to continue as there are no more predictions to consider
        if(i >= len(sortedSharkIDScoreList)): break
        # Extract ith tuple
        currentTuple = sortedSharkIDScoreList[i]
        # Get the shark ID
        currentPredID = currentTuple[0]
        currentScore = currentTuple[1]
        # Append this to the top K predictions
        # topKPredictedIDs.append(currentPredID)

        # We increase the rank of the correct prediction for each equivalence we find
        # So if there are many predictions with the same score to the correct prediction
        # we are "lowering" its rank to not consider it as much 
        # (where rank 0 is the highest)
        rank = -1

        # # If we're dealing with the true one
        # if(currentPredID == trueSharkID):
        #   # Go through all pairs
        #   for idx,scoreSharkIDPair in enumerate(sortedSharkIDScoreList):
        #     # If there is an equivalent score
        #     if(scoreSharkIDPair[1] == currentScore):
        #       rank = idx
        #       break

        # If the current predictedID we are considering is the trueSharkID
        if(currentPredID == trueSharkID):
          # Compare the correct prediction's score to all other scores
          for idx,scoreSharkIDPair in enumerate(sortedSharkIDScoreList):
            # If there is an equivalence in score
            if(scoreSharkIDPair[1] == currentScore):
              # If the rank hasn't been initialised, 
              # set it to the lowest index of this score
              if(rank == -1): 
                rank = idx + 1
              # If the rank has been set, increment
              else:
                # Increment the rank
                # Note, this will occur at least once as we compare it to itself
                rank = rank + 1
          # If the rank has exceed the number k we wanted to look at, don't count it
          if(rank <= self.k):
            # We increment, and then we don't care about the rest of the k's
            self.numberCorrect = self.numberCorrect + 1
            break

  # Return a dictionary of the final result
  def evaluate(self):
    # save self.count somewhere, or print it, or return it.
    accuracy = float(self.numberCorrect) / float(self.totalNumber)
    return {"total_num": self.totalNumber, "num_correct": self.numberCorrect, "accuracy": accuracy, "k": self.k}

'''


KAccDict = OrderedDict()
# Evaulate Accuracy
for i in range(1,11,2):
  accResult = EvaluateTestTopKAccuracy(i)
  k = accResult["k"]
  key = "top_"+str(k)+"_acc"
  KAccDict[key] = accResult

evaluationDict["acc"] = KAccDict

torch.save(evaluationDict,cfg.OUTPUT_DIR+"/evaluationDictionary.pt")

'''
def EvaluateTrainTopKAccuracy(numK):
  # Create evaluator object
  topKEvaluator = TopKAccuracy(numK)

  train_loader = build_detection_test_loader(cfg, "shark_train", mapper=mapper)
  # Get the accuracy results
  accuracy_results = inference_on_dataset(trainer.model, train_loader, topKEvaluator)
  # Extract results
  total_num   = str(accuracy_results["total_num"])
  num_correct = str(accuracy_results["num_correct"])
  top_k_acc   = str(round((accuracy_results["accuracy"]*100),2)) + "%"
  k           = str(accuracy_results["k"])

  # Create the string we're going to add to the text_file
  appendString = "\n________________________________________________________" \
               + "\nNumber correct: \t" + num_correct \
               + "\nTotal Number: \t\t" + total_num \
               + "\nTop " + str(k) + " Accuracy: \t" + top_k_acc \
               + "\n"

  # Append to the file
  text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
  text_file.write(appendString)
  text_file.close()

  # Print the file
  print(  "\nNumber correct: \t" + num_correct \
        + "\nTotal Number: \t\t" + total_num \
        + "\nTop " + str(k) + " Accuracy: \t" + top_k_acc \
        + "\n\n")

  # result = OrderedDict()
  # result["accuracy"]    = round(accuracy_results["accuracy"]*100,2)
  # result["num_correct"] = accuracy_results["num_correct"]
  # result["total_num"]   = accuracy_results["total_num"]
  # result["k"]           = accuracy_results["k"]
  # return result
'''

# Create the string we're going to add to the text_file
appendString = "\n________________________________________________________" \
              + "\nEvaluating the performance on training dataset" \
              + "\n"

# Append to the file
text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
text_file.write(appendString)
text_file.close()

for i in range(1,11,2):
  EvaluateTrainTopKAccuracy(i)

# Create the string we're going to add to the text_file
appendString = "\n________________________________________________________" \
              + "\n"

# Append to the file
text_file = open(cfg.OUTPUT_DIR+"/parameters-information.txt", "a+")
text_file.write(appendString)
text_file.close()


############### END Evaluation ###############

############### CSV Nightmare ###############
import csv
with open(cfg.OUTPUT_DIR+"/output.csv", "w") as outputCSV:
  writer = csv.writer(outputCSV)
  for key,value in evaluationDict.items():
    writer.writerow([key,value])


def AppendToCSV():
  paramKeys = list(evaluationDict["params"].keys())
  paramVals = list(evaluationDict["params"].values())

  cocoKeys = list(evaluationDict["coco"].keys())
  cocoKeys = cocoKeys[:len(cocoKeys)-1]
  cocoVals = list(evaluationDict["coco"].values())
  cocoVals = cocoVals[:len(cocoVals)-1]

  accVals = []
  for acc in list((evaluationDict["acc"].values())):
    accVals.append(acc["accuracy"])

  accKeys = list(evaluationDict["acc"].keys())

  perClass = evaluationDict["coco"].pop("PerClassAP")

  perClassKeyList = []
  perClassValList = []
  for key,value in perClass.items():
    perClassKeyList.append(key)
    perClassValList.append(value)

  cocoKeys = cocoKeys + perClassKeyList
  cocoVals = cocoVals + perClassValList

  resultKeys = paramKeys + accKeys + cocoKeys
  resultVals = paramVals + accVals + cocoVals

  # Creation of the csv and adding new keys will be done in a different script
  # Append new values
  csvFilename = "result_"+dataset_used+".csv"
  with open(csvFilename,"a+") as outputCSV:
    writer = csv.writer(outputCSV)
    # writer.writerow(resultKeys)
    writer.writerow(resultVals)

############### END CSV Nightmare ###############

AppendToCSV()

### Move the Slurm file ###
# Get the jobname
jobName = str(parser.parse_args().jobid)
print("Moving ",jobName)
# Create the file name
filename = "slurm-"+jobName+".out"
# Copy the file
shutil.copy("/mnt/storage/home/ja16475/sharks/detectron2/"+filename, cfg.OUTPUT_DIR+"/"+filename)
# Delete the original 
os.remove("/mnt/storage/home/ja16475/sharks/detectron2/"+filename)